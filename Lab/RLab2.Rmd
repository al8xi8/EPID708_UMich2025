---
title: "SSE 708: Machine Learning in the Era of Big Data"
subtitle: "Introductory to Machine Learning in R"
author: "David McCoy"
date: "July 7-11, 2025"
output: pdf_document
bibliography: references.bib
editor_options:
  chunk_output_type: console
---

```{r setup,  include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



First we look at basic commands, including simple graphics

# R Variable Types, structures, and objects

Variable types: 
- character
- numeric (real or decimal)
- integer
- logical
- complex

R Objects: 
- Data structures
- Functions

Data structures: 
- atomic vector
- list
- matrix
- data frame
- factors

 
First, you can run all the code at once in the code blocks by clicking the green arrow or you can select chunks of code and  use `cmd + enter` (Mac) or `control + enter` (PC)


```{r basic}
######################################################
############## Basic commands  ###################
########################################################
# Create Vectors
x <- 1:4
x <- c(1, 3, 2, 5)
x <- c(1, 7:9)
x <- seq(1:10)
x <- seq(1, 10, by = 2)
?seq
?c
x <- c("hi", "good", "bye")
x <- c(1:5, "hi", 9)

### Look at object
x
x <- c(1, 6, 2)
x
y <- c(1, 4, 3)

### number of elements in vector
length(x)
length(y)

### arithmetric operations on vectors
sqrt(x)
x^2
x + y
x * y
2 * y


### objects in directory in position 1
ls()

### Deleting objects from working directory
rm(x, y)
ls()

### Deleting ALL objects from working directory
rm(list = ls())

### getting help
?matrix

### Creating matrix and matrix operations
x <- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)
x
x <- matrix(c(1, 2, 3, 4), 2, 2)
matrix(c(1, 2, 3, 4), 2, 2, byrow = TRUE)
x <- matrix(c(1:10), 5, 2)
class(x) # look at the attributes of R objects
?class
# arithmetric operations on matrix
sqrt(x)
x^2
d <- function(x, y) {
  x + y
}
d(1, 2)
```


### Random number generators

```{r}
# Continuous variable
x <- rnorm(50) # normal distribution
?rnorm
y <- x + rnorm(50, mean = 50, sd = .1)
cor(x, y)
X <- runif(50, -2, 1) # uniform dist
?runif

# Binary variable
rbinom(10, 1, 0.5) # binomial dist
?rbinom()

# Categorical variable
sample(c(0:4), 100, replace = TRUE, prob = c(0.2, 0.25, 0.15, 0.3, 0.1))

## Good practice is to set seed so can duplicate results in future
set.seed(1303)
rnorm(50)
set.seed(3)
y <- rnorm(100)

### Statistical functions
mean(y)
var(y)
sqrt(var(y))
sd(y)
```

# Graphics
Now we look at graphics briefly.  Note, there are an enormouse selection of graphic options in R, including even interaction plots and the much praised ggplot [see @Wickham:2009aa]. 

```{r basic2}
### Generate random normal data.
x <- rnorm(100)
y <- rnorm(100)
## scatter plot
plot(x, y)
plot(x, y, xlab = "this is the x-axis", ylab = "this is the y-axis", main = "Plot of X vs Y")
## writing figure to file
pdf("Figure.pdf")
plot(x, y, col = "green")
dev.off()
## sequence of integers for simulating data for 3-D plots
x <- seq(1, 10)
x
x <- 1:10
x
x <- seq(-pi, pi, length = 50)
y <- x
#### Create matrix of positions for plotting function
f <- outer(x, y, function(x, y) cos(y) / (1 + x^2))
### Countour plot
contour(x, y, f)
contour(x, y, f, nlevels = 45, add = T)
fa <- (f - t(f)) / 2
contour(x, y, fa, nlevels = 15)
## Heatmap
image(x, y, fa)
## 3-D plot
persp(x, y, fa)
persp(x, y, fa, theta = 30)
persp(x, y, fa, theta = 30, phi = 20)
persp(x, y, fa, theta = 30, phi = 70)
persp(x, y, fa, theta = 30, phi = 40)
```

# Indexing, subsetting elements of data (vectors, matrices, data frames)
A common task for data management or viewing, etc., is to have to examine subsets of data elements.  

```{r subset}
######################################################################
################################# Indexing
######################################################################
A <- matrix(1:16, 4, 4)
A
## return 2nd row and 3rd column of A
A[2, 3]
## specified rows and columns
A[c(1, 3), c(2, 4)]
A[1:3, 2:4]
## all columns of first two rows
A[1:2, ]
## all rows of first two columns
A[, 1:2]
## remove 1st and 3rd row
A[-c(1, 3), ]
##
A[-c(1, 3), -c(1, 3, 4)]
dim(A)
```

# Reading in Data
Either using the base installation or by adding additional packages, one can read many different forms of data into R.

```{r dataread}
######################################################################
################################# Loading Data
######################################################################
## Change directory on computer to place where data is located
## (Note, can you "Session/Set Working Directory" menu item in Rstudio instead)
## Below is an example of changing directory - easiest to put
## data in same directory/folder as the source file
### Formatted data in general can be read using the read.table function
###  though subcommands such as read.csv are more specific
###  implementations
library(readr)
Auto <- read_csv("Auto.csv")
dim(Auto)
names(Auto) # Name of variables
head(Auto) # or click the data on the global environment
### Make sure to use first row as rownames and indicate sybmol that represents
### missing observation
# Auto <- read.table("Auto.data", header = T, na.strings = "?")
### Commands many types of data
Auto <- read.csv("Auto.csv", header = T, na.strings = "?")
Auto[1:4, ]
### Get rid of any rows that have any missing values
Auto <- na.omit(Auto)
### Note below - 5 rows had one or more missing values
dim(Auto)
### get a subset of the data using index
Auto[1:5, 1:4]

# Additional Graphical and Numerical Summaries
#### Try this (it won't work)
# plot(cylinders, mpg)
### Does not work, because no objects in directory of that name - need
### to reference these variables within Auto data frame
plot(Auto$cylinders, Auto$mpg)
### Can also move all variables in position 2 so they can be called by
### name directory.  In plain language, if you attach a data.frame, you
### can refer to the columns (variables) in it by using the name without the data.frame prefix$....
attach(Auto)
### Get list of variable names in Auto after attaching
ls(pos = 2)
plot(cylinders, mpg)
cylinders <- as.factor(cylinders)
plot(cylinders, mpg)
plot(cylinders, mpg, col = "red")
plot(cylinders, mpg, col = "red", varwidth = T)
plot(cylinders, mpg, col = "red", varwidth = T, horizontal = T)
plot(cylinders, mpg, col = "red", varwidth = T, xlab = "cylinders", ylab = "MPG")
hist(mpg)
hist(mpg, col = 2)
hist(mpg, col = 2, breaks = 15)
### Do all possible pair of scatter plots
##### removing the 9th column because it's a string
##### variable
pairs(Auto[, -9])
# pairs(Auto)
### only subset of them
pairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)
### Plot then identify points in plot
plot(horsepower, mpg)
# identify(horsepower,mpg,name)
detach(2)
```

# Data Summaries

You can summarize the elements of a data frame and all variables at once depending on whether you do so by the variable or the data.frame.

```{r summvar}
Auto <- read_csv("Auto.csv")
summary(Auto)
attach(Auto)
summary(mpg)
### Remove Auto from position 2 (can no longer reference variables just by name)
detach(2)
```

# Installation of packages
Packages that can be added to R are in a few places, mainly CRAN (ref), github and Bioconductor.  There are variations on the install.packages command, depending on whether one is loading package from local file versus straight from the CRAN webiste (the default), or from github (a package is necessary for installing from github, call devtools, and within that there is a function called
install_github.  Packages can be "dependent" on other packages and you can tell the install.packages function to also update or install dependencies.

```{r install, eval=FALSE}
### To read in stata data
install.packages("foreign", dependencies = TRUE)
### SuperLearner
install.packages("SuperLearner", dependencies = TRUE)
### TMLE
install.packages("tmle", dependencies = TRUE)
### Data exploration and filtering tool
install.packages("dplyr", dependencies = TRUE)
### Example Data package
install.packages("ISLR", dependencies = TRUE)
### Car package (Functions and Datasets to Accompany J. Fox and S. Weisberg, An R Companion to Applied Regression, Second Edition, Sage, 2011)
install.packages("car", dependencies = TRUE)
### VCD package
install.packages("vcd", dependencies = TRUE)
### rms
install.packages("rms", dependencies = TRUE)
### Tree Regression
install.packages("tree", dependencies = TRUE)
### GAM
install.packages("gam", dependencies = TRUE)
### Random Forest
install.packages("randomForest", dependencies = TRUE)
### polspline
install.packages("polspline", dependencies = TRUE)
### glmnet (LASSO)
install.packages("glmnet", dependencies = TRUE)
### SIS
install.packages("SIS", dependencies = TRUE)
### BayesTree
install.packages("BayesTree", dependencies = TRUE)
### gbm
install.packages("gbm", dependencies = TRUE)
### arm
install.packages("arm", dependencies = TRUE)
### ROCR
install.packages("ROCR", dependencies = TRUE)
### cvAUC
install.packages("cvAUC", dependencies = TRUE)
### xtable (writing out latex tables)
install.packages("xtable", dependencies = TRUE)
```

## Exercises
1. Generate a random normal sample of size 200 with mean 3 and standard deviation 1.
2. Get average (mean).
3. Repeat with sample size of 2,000. Are you closer to true mean this time? (This is not guaranteed but likely.)
4.  Generate a random Uniform X between -1 and 1 of sample size 500.
5.  Generate a random Y = 2+3X+e, where e is normal(mean=0,SD=2).
6.  Generate scatter plot of Y vs. X.
7.  Repeat with e being normal(mean=0,SD=0.5) --- what looks different?
8. Generate a 3x3 matrix of random normals.
9. Display the 3rd row.
10. Display all but 2nd column.
11. Make a histogram of weight in Auto data (Auto.csv).
12. Overlay the best fitting normal distribution (this might take a little research).

# Standard Regressions --- Parametric methods
We now review implementation of regression in R starting with linear regression moving on from there. 

## Linear
```{r linearmod}
library(MASS)

# Simple Linear Regression

names(Boston) # dataset from the MASS package
head(Boston)
?Boston # more descriptions about the boston dataset
### Linear fit, but won't work because must link to data
# lm.fit=lm(medv~lstat)
### Linear fit - now loads data
attach(Boston)
plot(lstat, medv)
lm.fit <- lm(medv ~ lstat, data = Boston)
### Different summaries of obj created
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
class(lm.fit)
lm.fit$coefficients
## Prediction
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")

# CONFIDENCE vs PREDICTION INTERVALS
# 
# Confidence Interval (interval = "confidence"):
# - Estimates the range for the AVERAGE/MEAN response at given predictor values
# - Answers: "Where is the true population mean likely to be?"
# - Only accounts for uncertainty in estimating the regression line
# - Narrower interval
# - Use when: Estimating mean response for planning/policy decisions
#
# Prediction Interval (interval = "prediction"):  
# - Estimates the range for a SINGLE NEW OBSERVATION at given predictor values
# - Answers: "Where is an individual new data point likely to fall?"
# - Accounts for BOTH regression line uncertainty AND natural variability of observations
# - Wider interval (always wider than confidence interval)
# - Use when: Predicting individual cases (e.g., "What will this specific house sell for?")
#
# Example: If lstat = 10
# Confidence: "Average home value for areas with lstat=10 is likely 20.5-22.1"
# Prediction: "A single home in area with lstat=10 is likely valued 15.2-27.4"

##### Plot Results
attach(Boston)
plot(lstat, medv)
abline(lm.fit)
abline(lm.fit, lwd = 3, col = "red")
?abline

### Different examples of symbols one could use
plot(lstat, medv, col = "red")
plot(lstat, medv, pch = 20)
plot(lstat, medv, pch = "+")
### example of symbols
plot(1:20, 1:20, pch = 1:20)
### Mulitplots on same page
par(mfrow = c(2, 2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
### Fing the row of maximum predicted value
which.max(hatvalues(lm.fit))
detach(2)

# Multiple Linear Regression
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
## Include all variables not medv in regression
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
library(car)
### Variance Inflaiton Factor (VIF)
vif(lm.fit)
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)

# VARIANCE INFLATION FACTOR (VIF)
#
# What is VIF?
# - Measures how much the variance of a coefficient increases due to multicollinearity
# - Multicollinearity = when predictor variables are highly correlated with each other
# - High multicollinearity makes coefficient estimates unstable and hard to interpret
#
# How to interpret VIF values:
# - VIF = 1: No correlation with other predictors (ideal)
# - VIF = 1-5: Moderate correlation (generally acceptable)
# - VIF = 5-10: High correlation (concerning, consider removing variable)
# - VIF > 10: Very high correlation (problematic, should remove variable)
#
# Why check VIF?
# - Identifies redundant predictors that don't add unique information
# - Helps create more stable, interpretable models
# - Prevents overfitting from highly correlated variables
#
# The code above:
# 1. Calculates VIF for all predictors in the full model
# 2. Creates new model removing 'age' (likely had high VIF)
# 3. Shows summary of improved model without the problematic predictor

### Fit model by adding or removing terms from current model
# MODEL MODIFICATIONS AND TRANSFORMATIONS

# UPDATE FUNCTION
# Alternative syntax to modify existing models without retyping everything
# update(original_model, new_formula) - more efficient than writing full lm() again
lm.fit1 <- update(lm.fit, ~ . - age)  # Remove 'age' from all predictors
summary(lm.fit1)

# INTERACTION TERMS
# 
# lstat * age = includes lstat + age + lstat:age (main effects + interaction)
# - Tests if the effect of lstat on medv DEPENDS ON the level of age
# - Example: Maybe lstat matters more for older neighborhoods
summary(lm(medv ~ lstat * age, data = Boston))

# lstat:age = includes ONLY the interaction term (no main effects)
# - Rarely used in practice - usually want main effects too
summary(lm(medv ~ lstat:age, data = Boston))

# NON-LINEAR TRANSFORMATIONS
#
# Adding polynomial terms to capture curved relationships
# I(lstat^2) - the I() function protects the ^2 from being interpreted as formula syntax
# Model becomes: medv = β₀ + β₁(lstat) + β₂(lstat²) + ε
lm.fit2 <- lm(medv ~ lstat + I(lstat^2), data = Boston)
summary(lm.fit2)

# MODEL COMPARISON VIA ANOVA
#
# Tests if the more complex model (with lstat²) significantly improves fit
# H₀: The quadratic term adds no value (simpler model is adequate)
# H₁: The quadratic term significantly improves the model
# Low p-value = reject H₀ = quadratic term is worthwhile
lm.fit <- lm(medv ~ lstat, data = Boston)
anova(lm.fit, lm.fit2)

# DIAGNOSTIC PLOTS
# Check assumptions of the quadratic model:
# 1. Residuals vs Fitted: Should show no pattern (randomness around 0)
# 2. Normal Q-Q: Points should follow diagonal line (normality of residuals)  
# 3. Scale-Location: Should show roughly horizontal line (constant variance)
# 4. Residuals vs Leverage: Identifies influential outliers (Cook's distance)
par(mfrow = c(2, 2))
plot(lm.fit2)

# ORTHOGONAL POLYNOMIALS
#
# poly(lstat, 5) creates 5th-degree polynomial with orthogonal terms
# - Orthogonal = polynomial terms are uncorrelated (better numerical stability)
# - Each term tests for significance of that polynomial degree
# - More flexible than manual I(lstat^2), I(lstat^3), etc.
# - Careful: higher degrees can lead to overfitting
lm.fit5 <- lm(medv ~ poly(lstat, 5), data = Boston)
summary(lm.fit5)
plot(lm.fit5)

# Qualitative Predictors
library(ISLR)
names(Carseats)
?Carseats
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
lm.fit <- lm(Sales ~ . + Income * Advertising + Price * Age, data = Carseats)
summary(lm.fit)
## Look at dummy variables related to ShelveLoc
attach(Carseats)
contrasts(ShelveLoc)
```

## Exercises
13. Fit linear model with Boston data of medv versus tax, zn, indus, chas, rad, and lstat.  
14. Predict the outcome on same data used to fit.
15. Plot observed versus predicted.


## Logistic Regression
Look at regression of binary outcomes versus predictors.
```{r logistic}
library(foreign)
library(vcd)
library(gam)
library(rms)
### Read in WCGS data
# Below is just for me.  Easiest just to put all data files
# used in same directory as the rmd file.  Use settings to set
# directory under
# Session/Set Working Directory/To Source File Location
wcgs <- read.dta("wcgs.dta")
names(wcgs)
head(wcgs)
summary(wcgs)

## Create Grouped Weight (0 to 4) represents wt categories
attach(wcgs)
class(weight0)
catwt <- weight0
head(catwt)
summary(catwt)
catwt[weight0 <= 150] <- 0
# head(catwt)
catwt[weight0 > 150 & weight0 <= 160] <- 1
catwt[weight0 > 160 & weight0 <= 170] <- 2
catwt[weight0 > 170 & weight0 <= 180] <- 3
catwt[weight0 > 180] <- 4
class(catwt)
factwt <- factor(catwt)
class(factwt)
# Auto <- read.table("Auto.data")
namewts <- c("<=150", "150-160", "160-170", "170-180", ">180")
factwt <- factor(catwt, labels = namewts)

### association between chd and dibpat0 using saturated model/nonparametric methods
# chd: coronary heart disease; dibpat0: behavior (type A vs type B personality)
table(chd69, dibpat0)
## Use oddsratio function in vcd
oddsratio(table(chd69, dibpat0), log = FALSE)

table(chd69, dibpat0, factwt)
ors <- oddsratio(table(chd69, dibpat0, factwt), log = FALSE)
summary(ors)
confint(ors)
plot(ors, xlab = "Wt(Cat)")

mantelhaen.test(chd69, dibpat0, factwt)

######################################  Logistic Regression
#### CHD vs. dibpat
######################################
# wcgs_new=cbind(wcgs,factwt)
glm.wt <- glm(chd69 ~ dibpat0 + factwt, data = wcgs, na.action = na.omit, family = binomial())
summary(glm.wt)
exp(coef(glm.wt))
#### CHD vs. Weight
glm.wt <- glm(chd69 ~ weight0, data = wcgs, na.action = na.omit, family = binomial())
summary(glm.wt)
exp(coef(glm.wt))
exp(confint(glm.wt))
lwr95ci <- exp(glm.wt$coef[2] - 1.96 * summary(glm.wt)$coef[2, 2])
upr95ci <- exp(glm.wt$coef[2] + 1.96 * summary(glm.wt)$coef[2, 2])

exp(20 * glm.wt$coef[2])
lwr95ci <- exp(20 * (glm.wt$coef[2] - 1.96 * summary(glm.wt)$coef[2, 2]))
upr95ci <- exp(20 * (glm.wt$coef[2] + 1.96 * summary(glm.wt)$coef[2, 2]))

## Function to get CI for Odds Ratio for a "mult" change in
## predictors, and allow for robust estimation of variance
lreg.or <- function(glm.mod, robust=FALSE, mult=NULL)
                    # robust indicates sandwich estimator
                    # mult is the OR for the "mult" change in the corresponding
#      predictors
{
  if (robust == TRUE) {
    glm.1 <- robcov(glm.mod)
    se <- sqrt(diag(glm.1$var))
    cf <- glm.1$coefficients
    lreg.coeffs <- cbind(cf, se)
  }
  if (robust == FALSE) {
    lreg.coeffs <- coef(summary(glm.mod))
  }
  p <- dim(lreg.coeffs)[1]
  if (is.null(mult)) {
    mult <- rep(1, p - 1)
  }
  l95ci <- exp(mult * (lreg.coeffs[2:p, 1] - 1.96 * lreg.coeffs[2:p, 2]))
  or <- exp(mult * lreg.coeffs[2:p, 1])
  u95ci <- exp(mult * (lreg.coeffs[2:p, 1] + 1.96 * lreg.coeffs[2:p, 2]))
  pvalue <- (2 * (1 - pnorm(abs(lreg.coeffs[, 1] / lreg.coeffs[, 2]))))[2:p]

  lreg.or <- cbind(l95ci, or, u95ci, pvalue)
  lreg.or
}

######  Get OR for a 20 lb change in weight
lreg.or(glm.wt, mult = 20)


###### Adjusted Models
#### Model 1
glm.mod1 <- glm(chd69 ~ dibpat0 + factwt, family = binomial(), data = wcgs, na.action = na.omit)
summary(glm.mod1)
lreg.or(glm.mod1)

#### Model 2
glm.mod2 <- glm(chd69 ~ dibpat0 * factwt, family = binomial(), data = wcgs, na.action = na.omit)
summary(glm.mod2)

#### OR's of Behavior at different weights
##### This is an analogous function to
#### estimate command in SAS or lincom in STATA
glm.post.estimate <- function(glmob, comps, exponentiate = TRUE) {
  if (is.matrix(comps) == FALSE) {
    comps <- t(as.matrix(comps))
  }
  vc <- vcov(glmob)
  ests <- coef(glmob)
  linear.ests <- as.vector(comps %*% ests)
  vcests <- comps %*% vc %*% t(comps)
  ses <- sqrt(diag(vcests))
  pvalue <- (2 * (1 - pnorm(abs(linear.ests / ses))))
  if (exponentiate) {
    l95ci <- exp(linear.ests - 1.96 * ses)
    or <- exp(linear.ests)
    u95ci <- exp(linear.ests + 1.96 * ses)
    summ <- cbind(l95ci, or, u95ci, pvalue)
  }
  if (exponentiate == FALSE) {
    l95ci <- (linear.ests - 1.96 * ses)
    logor <- (linear.ests)
    u95ci <- (linear.ests + 1.96 * ses)
    summ <- cbind(l95ci, logor, u95ci, pvalue)
  }
  return(summ)
}

comps <- rbind(
  c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0),
  c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0),
  c(0, 1, 0, 0, 0, 0, 0, 1, 0, 0),
  c(0, 1, 0, 0, 0, 0, 0, 0, 1, 0),
  c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1)
)

glm.post.estimate(glm.mod2, comps)

#### Test of interaction (Model 2 vs 1)
anova(glm.mod2, glm.mod1, test = "Chisq")
```

## Exercises
16. Do a logistic regression using wcgs data of chd69 versus dippat0, weight0, sbp0, dpb0, chol0, age0, and arcus0.
17. Predict the outcome on the data. 
18. Using different cut-offs on the predicted probability (e.g., > 0.5, predict 1, otherwise 0), look at predicted outcomes versus observed using table() function.



# Standard Regressions --- Semiparametric methods
We depart from standard generalized linear models to more flexible,  more data-adaptive techniques for regression.

## Generalized Additive Models (GAMs)
We now look at approaches based on so-called smooth regression

```{r smooth}
###### Look at Smooth logistic regression for WCGS Data
library(gam)
gam.wt <- gam(chd69 ~ lo(weight0), data = wcgs, na.action = na.omit, family = binomial())
summary(gam.wt)
wt <- seq(90, 310, length = 50)
pred.wt <- predict(gam.wt, newdata = data.frame(weight0 = wt), type = "response")
par(oma = c(4, 2, 1, 1))
plot(wt, pred.wt, type = "l", main = "Loess Smooth WCGS", xlab = "Weight(x)", ylab = "P(Y=1|X=x)")


# GAM using Wage Data
library(ISLR)
library(gam)
attach(Wage)
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)
# Select plot you wish to see, 0 to exit
plot.Gam(gam1, se = TRUE, col = "red")
gam.m1 <- gam(wage ~ s(age, 5) + education, data = Wage)
gam.m2 <- gam(wage ~ year + s(age, 5) + education, data = Wage)
anova(gam.m1, gam.m2, test = "F")
```

# Semiparametric approaches to regression
We depart from standard generalized linear models to more flexible,  more data-adaptive techniques for regression.

## Generalized Generalized Additive Models (GAM)
We now look at approaches based on so-called smooth regression parametric smoothing
quadratic, cubic nonparametric smoothing loess, nature splines, b-splines, etc.

```{r smooth2}
###### Look at Smooth logistic regression for WCGS Data
library(gam) # Generalized Additive Models
gam.wt <- gam(chd69 ~ lo(weight0), data = wcgs, na.action = na.omit, family = binomial())
summary(gam.wt)
wt <- seq(90, 310, length = 50)
pred.wt <- predict(gam.wt, newdata = data.frame(weight0 = wt), type = "response")
par(oma = c(4, 2, 1, 1))
plot(wt, pred.wt, type = "l", main = "Loess Smooth WCGS", xlab = "Weight(x)", ylab = "P(Y=1|X=x)")

# GAM using Wage Data
library(ISLR)
library(gam)
attach(Wage) # Wage dataset is in the ISLR package
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)
# Select plot you wish to see, 0 to exit
plot.Gam(gam1, se = TRUE, col = "red")
gam.m1 <- gam(wage ~ s(age, 5) + education, data = Wage)
gam.m2 <- gam(wage ~ year + s(age, 5) + education, data = Wage)
anova(gam.m1, gam.m2, test = "F")
```

## Exercises
19. Do a smooth gam fit (using `family="binomial"`) of chd69 versus chol0. Plot results.
20. Repeat, but vary the smoothness by using the `df` option within `s()`.


## Tree Regression
### Classification Trees (factor as outcome)

```{r classtrees}
library(tree)
library(ISLR)
attach(Carseats)
View(Carseats)
class(Sales)
summary(Sales)
hist(Sales)
High <- ifelse(Sales <= 8, "No", "Yes")
# High=ifelse(Sales<=8,0,1)
# High=factor(High)
class(High)
Carseats <- data.frame(Carseats, High)
# added code
Carseats[sapply(Carseats, is.character)] <- lapply(Carseats[sapply(Carseats, is.character)], as.factor)
detach(2)
tree.carseats <- tree(High ~ . - Sales, Carseats)
summary(tree.carseats)
par(mfrow = c(1, 1))
plot(tree.carseats)
text(tree.carseats, pretty = 0, cex = 0.50)

### establish tree in training set first and then apply the tree to testing set
set.seed(2)
train <- sample(1:nrow(Carseats), 200) # half of the full dataset
Carseats.test <- Carseats[-train, ] # remove rows selected to form the train set and keep all variables
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)

### cross validation in training set
set.seed(3)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
# ?cv.tree
# names(cv.carseats)
# dev - error - lower is better
# k - the value of the cost-complexity pruning parameter of each tree in the sequence
cv.carseats
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")

# take previous tree and prune to the best number of leaves
prune.carseats <- prune.misclass(tree.carseats, best = 9)
par(mfrow = c(1, 1))
plot(prune.carseats)
text(prune.carseats, pretty = 0, cex = 0.5)

## prediction in test set
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)

## change size of the tree
prune.carseats <- prune.misclass(tree.carseats, best = 15)
plot(prune.carseats)
text(prune.carseats, pretty = 0, cex = 0.5)
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

## Exercises
21. Estimate and plot a tree of chd69 versus dippat0, weight0 sbp0 dpb0 chol0 age0 and arcus0.


### Regression trees (ordered outcome)
Now look at regression trees for ordered outcome (including continuous variable or ordinal categorical variable).
```{r regtree}
library(MASS)
?Boston
head(Boston$medv)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
test <- Boston[-train, ]
boston.test <- Boston[-train, "medv"] # outcome variable

tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0, cex = 0.5)

cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0, cex = 0.5)

yhat <- predict(tree.boston, test)

plot(yhat, boston.test)
abline(0, 1) # perfect prediction line
mean((yhat - boston.test)^2) # Mean squared error (MSE)
```

## Random Forest
Random Forest is a popular machine-learning tools based on a combination of "bagging" (bootstrap aggregation) and trees [see @Breiman01RF and @HTF01]. It reduces some of the noise variability from a single tree fit by averaging this noise across many trees.  It produces these trees by taking repeated samples of the data (with replacement), so by using the nonparametric bootstrap. Sevearl parts of the algorithm are "tuneable" that is can be altered by changing some tuning parameter (e.g., number of variables to use per tree, number of total trees to average, ...).

```{r randomF}
library(randomForest)
set.seed(1)
#### Fit RF to "train" subset of data, which is list of row numbers to include
# range(train)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE)
# mtry is Number of variables randomly sampled as candidates at each split
bag.boston
## Predict for data not in training set
yhat.bag <- predict(bag.boston, test)
### observed versus predicted values (boston.test is observed outcomes in test set)
par(mfrow = c(1, 1))
plot(yhat.bag, boston.test)
abline(0, 1)
## MSE
mean((yhat.bag - boston.test)^2)
## changing tuning parameters  (mtry is # of variables per tree, ntree is number of trees fit)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, ntree = 25)
yhat.bag <- predict(bag.boston, test)
## MSE for this fit
mean((yhat.bag - boston.test)^2)
set.seed(1)
### Variable importance returned from Random Forest
rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, test)
mean((yhat.rf - boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
```

## Exercises
22. Perform a random forest of the same outcome and predictors as 23. above and display variable importance. (Note, because there are some missing values, you need to add the `na.action=na.omit` option. Also, you need to define the outcome chd69 as a factor variable so that Random Forest will use the right loss function --- use `factor(chd69)~dibpat0+....` in the formula.


# References
